\section{Methods}
\label{sec:methods}

In this section, we formally define the problem of financial document understanding and present the HierFinRAG framework. As illustrated in Figure \ref{fig:arch}, our approach decomposes the task into three principal components: (1) Structure-Aware Graph Construction, (2) Table-Text Graph Learning, and (3) Symbolic-Neural Fusion.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Fig0_Architecture.png}
    \caption{Overview of the HierFinRAG framework. Phase 1 builds a heterogeneous graph from financial documents. Phase 2 retrieves relevant context using the Table-Text Graph Neural Network (TTGNN). Phase 3 routes queries to the optimal reasoning engine (Symbolic, Neural, or Hybrid) as detailed in Algorithm \ref{alg:routing}.}
    \label{fig:arch}
\end{figure}

\subsection{Problem Formulation}
Let $\mathcal{D} = \{S_1, S_2, \dots, S_N\}$ be a financial document composed of $N$ sections. Each section $S_i$ contains a set of text paragraphs $\mathcal{P}_i$ and tables $\mathcal{T}_i$. Key notations used throughout this paper are summarized in Table \ref{tab:notations}.

\begin{table}[h]
\centering
\begin{tabular}{l l}
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
$\mathcal{G}=(\mathcal{V}, \mathcal{E})$ & Heterogeneous Table-Text Graph \\
$\mathcal{V}_P, \mathcal{V}_T, \mathcal{V}_C$ & Paragraph, Table, and Cell nodes \\
$e_{struct}, e_{sem}$ & Structural and Semantic edges \\
$\mathbf{h}_i$ & Embedding vector for node $v_i$ \\
$\pi$ & Synthesized symbolic program \\
\hline
\end{tabular}
\caption{Summary of Notations.}
\label{tab:notations}
\end{table}

A table $T \in \mathcal{T}_i$ is defined as a matrix of cells $C_{r,c}$. Given a user query $Q$, the objective is to generate an answer $A$ and a set of supporting evidence $\mathcal{E} \subset \mathcal{D}$ such that $A$ is factually grounded in $\mathcal{E}$.

\subsection{Table-Text Graph Construction}
To capture the hierarchical and cross-modal dependencies, we construct a heterogeneous graph $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{R})$, where $\mathcal{V}$ is the set of nodes, $\mathcal{E}$ is the set of edges, and $\mathcal{R}$ denotes relation types.

\subsubsection{Node Representation}
The node set $\mathcal{V}$ comprises five distinct types: $\mathcal{V} = \mathcal{V}_P \cup \mathcal{V}_S \cup \mathcal{V}_T \cup \mathcal{V}_C \cup \mathcal{V}_G$, representing Paragraphs, Sections, Tables, Cells, and global document notes, respectively. Each node $v_i \in \mathcal{V}$ is initialized with a feature vector $\mathbf{h}_i^{(0)} \in \mathbb{R}^d$ via a pre-trained encoder (e.g., BERT-Large):
\begin{equation}
    \mathbf{h}_i^{(0)} = \text{Encoder}(\text{content}(v_i))
\end{equation}

\subsubsection{Edge Formation}
We define a set of semantic and structural relations $\mathcal{R}$ to capture explicit and implicit dependencies:
\begin{itemize}
    \item \textit{Structural Edges} ($e_{struct}$): Deterministic links reflecting the document hierarchy. We connect each cell $v_c$ to its column header $v_h$ and row header $v_{rh}$. Tables and paragraphs are linked to their parent Section $v_s$.
    \item \textit{Semantic Edges} ($e_{sem}$): Established between a paragraph $v_p$ and a table row $v_r$ to capture implicit alignment (e.g., a paragraph discussing "revenue growth" implicitly linking to the "2023 Revenue" row). We create an edge if $\text{sim}(\mathbf{h}_p, \mathbf{h}_r) > \tau$, where $\tau=0.75$ determines the density of semantic connectivity.
    \item \textit{Cross-Reference Edges} ($e_{ref}$): Explicit links detected via high-precision regex matching (e.g., patterns like \texttt{"(see Table \textbackslash d+)"} or \texttt{"As shown in Figure \textbackslash d+"}).
\end{itemize}

\subsection{Table-Text Graph Neural Network (TTGNN)}
We propose the Table-Text Graph Neural Network (TTGNN) to verify the alignment between textual claims and tabular data. The network employs a relational graph attention mechanism.

For a node $i$ and its neighbor $j$ under relation $r \in \mathcal{R}$, the attention coefficient $\alpha_{i,j}^r$ is computed as:
\begin{equation}
    \alpha_{i,j}^r = \frac{\exp(\sigma(\mathbf{a}_r^T [\mathbf{W}_r \mathbf{h}_i \| \mathbf{W}_r \mathbf{h}_j]))}{\sum_{k \in \mathcal{N}_i^r} \exp(\sigma(\mathbf{a}_r^T [\mathbf{W}_r \mathbf{h}_i \| \mathbf{W}_r \mathbf{h}_k]))}
\end{equation}
where $\mathbf{W}_r$ is a relation-specific weight matrix, $\mathbf{a}_r$ is the attention vector, and $\|$ denotes concatenation.

The node representations are updated across $L$ layers:
\begin{equation}
    \mathbf{h}_i^{(l+1)} = \sigma\left( \sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_i^r} \alpha_{i,j}^r \mathbf{W}_r \mathbf{h}_j^{(l)} \right) + \mathbf{h}_i^{(l)}
\end{equation}
We utilize \texttt{GATv2Conv} layers with multi-head attention (8 heads) and residual connections. Crucially, the edge relation type $r$ is embedded into a dense vector $\mathbf{e}_r \in \mathbb{R}^d$ and injected directly into the attention scoring function, allowing the model to weigh structural vs. semantic connections dynamically.

To optimize the graph representation, we employ a supervised contrastive loss $\mathcal{L}_{con}$. Let $(v_i, v_p)$ be a positive pair (aligned text and table cell) and $\mathcal{N}$ be a batch of negative samples:
\begin{equation}
    \mathcal{L}_{con} = -\log \frac{\exp(\text{sim}(\mathbf{h}_i, \mathbf{h}_p) / \tau)}{\sum_{ v_n \in \mathcal{N}} \exp(\text{sim}(\mathbf{h}_i, \mathbf{h}_n) / \tau)}
\end{equation}

\subsection{Hierarchical Attention Retrieval}
Retrieval operates in a top-down cascade.
\textbf{Level 1 (Coarse):} We retrieve relevant sections using dense passage retrieval. The relevance score $S_{sec}(Q, S_i)$ is:
\begin{equation}
    S_{sec}(Q, S_i) = \cos(\mathbf{q}, \mathbf{s}_i)
\end{equation}
\textbf{Level 2 (Fine):} Within top-$k$ sections, we employ Reciprocal Rank Fusion (RRF) to combine sparse (BM25) and dense scores for paragraphs and tables:
\begin{equation}
    RRF(d) = \sum_{r \in \mathcal{M}} \frac{1}{\kappa + \text{rank}_r(d)}
\end{equation}
where $\mathcal{M} = \{\text{BM25}, \text{Dense}\}$ and $d \in \mathcal{P}_i \cup \mathcal{T}_i$.

\subsection{Symbolic-Neural Fusion}
To handle numerical reasoning, we introduce a routing function $\Phi(Q, \mathcal{C})$ that classifies the query type given context $\mathcal{C}$.
\begin{equation}
    \text{Mode} = \operatorname{argmax}_{m \in \{\text{Neural}, \text{Symbolic}, \text{Hybrid}\}} P(m | Q, \mathcal{C})
\end{equation}
We term this mechanism \textbf{Probabilistic Hard-Routing}. Unlike soft-gating mixtures of experts, our router makes a hard decision to branch execution, optimizing for both accuracy and latency. The full routing logic is detailed in Algorithm \ref{alg:routing}.

If $\text{Mode} = \text{Symbolic}$, the system generates a pythonic program $\pi$. For example, to calculate a ratio:
\begin{equation}
    \pi = \texttt{divide}(\texttt{get\_cell}(T_{2,3}), \texttt{get\_cell}(T_{4,1}))
\end{equation}
The final execution result $V = \text{Exec}(\pi)$ is injected into the response template.

The overall confidence score $C$ is a weighted combination of retrieval relevance and reasoning probability:
\begin{equation}
    C = \lambda_1 \cdot \text{sigmoid}(S_{sec}) + \lambda_2 \cdot P(\pi | Q)
\end{equation}

This hybrid approach ensures that arithmetic operations are provably correct while maintaining natural language flexibility.

\begin{algorithm}[h]
\caption{Symbolic-Neural Fusion Routing}
\label{alg:routing}
\begin{algorithmic}[1]
\Require Query $q$, Retrieved Context $C = \{c_1, \dots, c_k\}$
\Ensure Answer $A$
\State $t_{context} \leftarrow \text{ExtractNodeTypes}(C)$ \Comment{e.g., \{Table, Text\}}
\State $score_{sym} \leftarrow \text{KeywordMatch}(q, \mathcal{K}_{math})$
\State $p_{sym} \leftarrow \sigma(w_1 \cdot score_{sym} + w_2 \cdot \mathbb{I}(\text{Table} \in t_{context}))$
\If{$p_{sym} > \tau_{high}$}
    \State $\mathcal{E} \leftarrow \text{ExtractVariables}(q, C)$
    \State $Prog \leftarrow \text{SynthesizeProgram}(q)$
    \State $A \leftarrow \text{Execute}(Prog, \mathcal{E})$ \Comment{Symbolic Mode}
\ElsIf{$p_{sym} < \tau_{low}$}
    \State $A \leftarrow \text{LLM.Generate}(q, C)$ \Comment{Neural Mode}
\Else
    \State $Plan \leftarrow \text{LLM.Plan}(q)$
    \State $Val \leftarrow \text{Execute}(Plan.math)$
    \State $A \leftarrow \text{LLM.Synthesize}(Plan.text, Val)$ \Comment{Hybrid Mode}
\EndIf
\Return $A$
\end{algorithmic}
\end{algorithm}

