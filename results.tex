\section{Experimental Results}
\label{sec:results}

In this section, we present a comprehensive evaluation of the HierFinRAG framework. Our experiments are designed to answer three key research questions: (1) Does incorporating hierarchical structure and symbolic reasoning improve accuracy on complex financial queries? (2) How effective is the graph-based retrieval mechanism compared to standard dense retrieval? (3) What are the efficiency implications of our modular approach in terms of latency and computational cost?

We evaluate our system on two primary datasets: \textbf{FinQA}, which focuses on numerical reasoning over financial tables, and \textbf{FinanceBench}, which requires retrieving evidence from long-form unformatted documents (e.g., 10-K filings). We compare HierFinRAG against strong baselines including Vanilla RAG (using OpenAI's text-embedding-3-large) and GPT-4o equipped with a Code Interpreter environment.

\subsection{Comparative Analysis on Financial Benchmarks}
Figure \ref{fig:performance} illustrates the Exact Match (EM) accuracy on FinQA and Numerical Accuracy on FinanceBench. HierFinRAG establishes a new state-of-the-art on both benchmarks.

On \textbf{FinQA}, our model achieves an accuracy of \textbf{82.5\%}, surpassing the GPT-4o Code Interpreter (76.0\%) by 6.5 percentage points. This performance gain is largely attributable to the \textit{Symbolic-Neural Fusion} module. While GPT-4o occasionally hallucinates during multi-step arithmetic generation, our system's ability to map numerical intents to a deterministic calculator ensures arithmetic precision.

The performance gap is even more pronounced on \textbf{FinanceBench}, where HierFinRAG attains \textbf{74.0\%} accuracy compared to 48.0\% for GPT-4o and just 32.5\% for Vanilla RAG. FinanceBench documents are notoriously lengthy and structurally complex. Standard RAG systems often retrieve irrelevant chunks due to keyword overlap, missing the subtle connections between a table row and a footnote several pages away. Our \textit{Table-Text Graph Neural Network (TTGNN)} explicitly models these connections, allowing the retrieval algorithm to traverse structural edges (e.g., Table $\rightarrow$ Section $\rightarrow$ Footnote) and aggregate scattered evidence, thereby significantly reducing hallucinations caused by missing context.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Fig1_Main_Performance.png}
    \caption{Comparative performance of HierFinRAG versus Vanilla RAG and GPT-4o Code Interpreter. Our multimodal approach demonstrates substantial gains, particularly on the long-context FinanceBench dataset.}
    \label{fig:performance}
\end{figure}

\subsection{Retrieval Efficacy}
The backbone of any RAG system is its retrieval component. We analyze the specific contribution of our hierarchical attention mechanism using the Recall@k metric, as shown in Figure \ref{fig:retrieval}.

HierFinRAG consistently outperforms baselines across all values of $k$. Crucially, at a strict threshold of $k=5$, our method achieves nearly \textbf{80\% recall}, whereas Vanilla RAG struggles to surpass 45\%. In financial QA, high precision at low $k$ is vital because the generator (LLM) is easily distracted by "financial noise"â€”statements that look numerically similar but refer to different years or entities. By leveraging the semantic edges in our graph, HierFinRAG filters out this noise effectively. For instance, when a query asks for "2023 Revenue," the specific edge connecting the "2023" column header to the cell value is prioritized over a generic paragraph discussing "Revenue goals."

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Fig2_Retrieval_Recall.png}
    \caption{Retrieval Recall@k Comparison. The steep learning curve of HierFinRAG indicates its ability to locate the correct evidence within the first few retrieved chunks, minimizing noise for the generator.}
    \label{fig:retrieval}
\end{figure}

\subsection{Efficiency and Deployment Feasibility}
Beyond raw accuracy, we evaluate the trade-off between system latency and performance. Figure \ref{fig:efficiency} plots the average latency per query against accuracy.

High-performing agentic systems like GPT-4o Code Interpreter suffer from high latency ($\sim$15s/query) due to the iterative nature of code generation and execution. Conversely, vision-based models like ColPali ($>$8s/query) are computationally expensive, processing high-resolution page images. HierFinRAG occupies the optimal "Pareto frontier," delivering \textbf{state-of-the-art accuracy with an average latency of just 4.2 seconds}. This 3.5x speedup over agentic baselines is achieved by offloading complex reasoning to the lightweight symbolic engine and using the GNN for efficient, non-iterative retrieval. Furthermore, our token consumption analysis (omitted for brevity) indicates a $\sim$40\% reduction in input tokens compared to Vanilla RAG, as the precise retrieval reduces the need to stuff the context window with irrelevant pages.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Fig3_Efficiency_Tradeoff.png}
    \caption{Efficiency vs. Accuracy analysis. HierFinRAG provides a scalable solution for real-time financial analysis, balancing high accuracy with low latency.}
    \label{fig:efficiency}
\end{figure}

\subsection{Error Distribution and Limitations}
To identify areas for future improvement, we conducted a manual analysis of 100 error cases from the validation set, categorized in Figure \ref{fig:error}.

\textbf{Retrieval failures (40\%)} remain the primary bottleneck. These often occur in "needle-in-a-haystack" scenarios where a critical number is buried in a non-standardized/unnamed table without clear headers. \textbf{Reasoning errors (30\%)} arise when the intent classifier misinterprets a complex query (e.g., asking for a "CAGR" but the system interprets it as simple "Growth"), leading to an incorrect symbolic program. Finally, \textbf{Generation (20\%)} and \textbf{Cross-Reference (10\%)} errors highlight the occasional failure of the LLM to synthesize the retrieved pieces into a coherent narrative. Addressing these retrieval gaps in ultra-sparse financial tables remains a key direction for our future work.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{Fig4_Error_Analysis.png}
    \caption{Distribution of Error Types. While reasoning and generation are robust, retrieval remains the largest challenge, motivating further research into structure-aware embedding models.}
    \label{fig:error}
\end{figure}
